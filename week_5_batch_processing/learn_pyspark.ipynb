{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/31 03:01:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master('local[*]') \\\n",
    "        .appName('test') \\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schemas and Creating DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yellow_schema = StructType(\n",
    "#     [\n",
    "#         StructField('VendorID', IntegerType()),\n",
    "#         StructField('lpep_pickup_datetime', TimestampType()),\n",
    "#         StructField('lpep_dropoff_datetime', TimestampType()),\n",
    "#         StructField('store_and_fwd_flag', StringType()),\n",
    "#         StructField('RatecodeID', DoubleType()),\n",
    "#         StructField('PULocationID', IntegerType()),\n",
    "#         StructField('DOLocationID', IntegerType()),\n",
    "#         StructField('passenger_count', IntegerType()),\n",
    "#         StructField('trip_distance', DoubleType()),\n",
    "#         StructField('fare_amount', DoubleType()),\n",
    "#         StructField('extra', DoubleType()),\n",
    "#         StructField('mta_tax', DoubleType()),\n",
    "#         StructField('tip_amount', DoubleType()),\n",
    "#         StructField('tolls_amount', DoubleType()),\n",
    "#         StructField('improvement_surcharge', DoubleType()),\n",
    "#         StructField('total_amount', DoubleType()),\n",
    "#         StructField('payment_type', DoubleType()),\n",
    "#         StructField('trip_type', DoubleType()),\n",
    "#         StructField('congestion_surcharge', DoubleType())\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# green_schema = StructType(\n",
    "#     [\n",
    "#         StructField('VendorID', IntegerType()),\n",
    "#         StructField('lpep_pickup_datetime', TimestampType()),\n",
    "#         StructField('lpep_dropoff_datetime', TimestampType()),\n",
    "#         StructField('store_and_fwd_flag', StringType()),\n",
    "#         StructField('RatecodeID', DoubleType()),\n",
    "#         StructField('PULocationID', IntegerType()),\n",
    "#         StructField('DOLocationID', IntegerType()),\n",
    "#         StructField('passenger_count', IntegerType()),\n",
    "#         StructField('trip_distance', DoubleType()),\n",
    "#         StructField('fare_amount', DoubleType()),\n",
    "#         StructField('extra', DoubleType()),\n",
    "#         StructField('mta_tax', DoubleType()),\n",
    "#         StructField('tip_amount', DoubleType()),\n",
    "#         StructField('tolls_amount', DoubleType()),\n",
    "#         StructField('ehail_fee', DoubleType()),\n",
    "#         StructField('improvement_surcharge', DoubleType()),\n",
    "#         StructField('total_amount', DoubleType()),\n",
    "#         StructField('payment_type', DoubleType()),\n",
    "#         StructField('trip_type', DoubleType()),\n",
    "#         StructField('congestion_surcharge', DoubleType())\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_green = spark.read.parquet('data/raw/green/*/*')\n",
    "df_yellow = spark.read.parquet('data/raw/yellow/*/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\n",
      "|VendorID|lpep_pickup_datetime|lpep_dropoff_datetime|store_and_fwd_flag|RatecodeID|PULocationID|DOLocationID|passenger_count|trip_distance|fare_amount|extra|mta_tax|tip_amount|tolls_amount|ehail_fee|improvement_surcharge|total_amount|payment_type|trip_type|congestion_surcharge|\n",
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\n",
      "|       2| 2019-12-18 15:52:30|  2019-12-18 15:54:39|                 N|       1.0|         264|         264|            5.0|          0.0|        3.5|  0.5|    0.5|      0.01|         0.0|     null|                  0.3|        4.81|         1.0|      1.0|                 0.0|\n",
      "|       2| 2020-01-01 00:45:58|  2020-01-01 00:56:39|                 N|       5.0|          66|          65|            2.0|         1.28|       20.0|  0.0|    0.0|      4.06|         0.0|     null|                  0.3|       24.36|         1.0|      2.0|                 0.0|\n",
      "|       2| 2020-01-01 00:41:38|  2020-01-01 00:52:49|                 N|       1.0|         181|         228|            1.0|         2.47|       10.5|  0.5|    0.5|      3.54|         0.0|     null|                  0.3|       15.34|         1.0|      1.0|                 0.0|\n",
      "|       1| 2020-01-01 00:52:46|  2020-01-01 01:14:21|                 N|       1.0|         129|         263|            2.0|          6.3|       21.0| 3.25|    0.5|       0.0|         0.0|     null|                  0.3|       25.05|         2.0|      1.0|                2.75|\n",
      "|       1| 2020-01-01 00:19:57|  2020-01-01 00:30:56|                 N|       1.0|         210|         150|            1.0|          2.3|       10.0|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|        11.3|         1.0|      1.0|                 0.0|\n",
      "|       1| 2020-01-01 00:52:33|  2020-01-01 01:09:54|                 N|       1.0|          35|          39|            1.0|          3.0|       13.5|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|        14.8|         1.0|      1.0|                 0.0|\n",
      "|       2| 2020-01-01 00:10:18|  2020-01-01 00:22:16|                 N|       1.0|          25|          61|            1.0|         2.77|       11.0|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|        12.3|         2.0|      1.0|                 0.0|\n",
      "|       2| 2020-01-01 01:03:14|  2020-01-01 01:29:45|                 N|       1.0|         225|          89|            1.0|         4.98|       20.5|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|        21.8|         2.0|      1.0|                 0.0|\n",
      "|       2| 2020-01-01 00:04:11|  2020-01-01 00:09:48|                 N|       1.0|         129|         129|            1.0|         0.71|        5.5|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|         6.8|         2.0|      1.0|                 0.0|\n",
      "|       2| 2020-01-01 00:25:52|  2020-01-01 00:32:16|                 N|       1.0|         129|          83|            1.0|          0.8|        5.5|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|         6.8|         2.0|      1.0|                 0.0|\n",
      "|       2| 2020-01-01 00:47:32|  2020-01-01 00:59:25|                 N|       1.0|          82|         173|            1.0|         1.52|        9.5|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|        10.8|         2.0|      1.0|                 0.0|\n",
      "|       1| 2020-01-01 00:26:40|  2020-01-01 00:40:42|                 N|       1.0|          74|          69|            1.0|          3.8|       14.0|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|        15.3|         2.0|      1.0|                 0.0|\n",
      "|       2| 2020-01-01 00:38:47|  2020-01-01 00:46:02|                 N|       1.0|          74|          41|            1.0|         1.12|        6.5|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|         7.8|         1.0|      1.0|                 0.0|\n",
      "|       2| 2020-01-01 00:52:18|  2020-01-01 01:09:58|                 N|       1.0|          41|         127|            1.0|         5.67|       19.0|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|        20.3|         2.0|      1.0|                 0.0|\n",
      "|       2| 2020-01-01 00:16:01|  2020-01-01 00:26:40|                 N|       1.0|           7|         260|            1.0|         1.86|        9.5|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|        10.8|         2.0|      1.0|                 0.0|\n",
      "|       2| 2020-01-01 00:21:15|  2020-01-01 00:28:03|                 N|       1.0|           7|           7|            1.0|         1.42|        7.0|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|         8.3|         2.0|      1.0|                 0.0|\n",
      "|       2| 2020-01-01 00:35:42|  2020-01-01 01:02:00|                 N|       1.0|           7|         133|            1.0|        15.48|       43.0|  0.5|    0.5|      8.86|         0.0|     null|                  0.3|       53.16|         1.0|      1.0|                 0.0|\n",
      "|       2| 2020-01-01 00:39:51|  2020-01-01 00:42:54|                 N|       1.0|         134|          28|            1.0|         1.15|        5.5|  0.5|    0.5|       1.0|         0.0|     null|                  0.3|         7.8|         1.0|      1.0|                 0.0|\n",
      "|       1| 2020-01-01 00:00:21|  2020-01-01 00:10:19|                 N|       1.0|          89|          39|            1.0|          2.3|       10.0|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|        11.3|         2.0|      1.0|                 0.0|\n",
      "|       1| 2020-01-01 00:13:59|  2020-01-01 00:21:31|                 N|       1.0|          66|          65|            3.0|          1.0|        6.5|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|         7.8|         2.0|      1.0|                 0.0|\n",
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_green.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: long (nullable = true)\n",
      " |-- lpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- lpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- RatecodeID: double (nullable = true)\n",
      " |-- PULocationID: long (nullable = true)\n",
      " |-- DOLocationID: long (nullable = true)\n",
      " |-- passenger_count: double (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- ehail_fee: integer (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- payment_type: double (nullable = true)\n",
      " |-- trip_type: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the schema used\n",
    "df_green.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('VendorID', LongType(), True), StructField('lpep_pickup_datetime', TimestampType(), True), StructField('lpep_dropoff_datetime', TimestampType(), True), StructField('store_and_fwd_flag', StringType(), True), StructField('RatecodeID', DoubleType(), True), StructField('PULocationID', LongType(), True), StructField('DOLocationID', LongType(), True), StructField('passenger_count', DoubleType(), True), StructField('trip_distance', DoubleType(), True), StructField('fare_amount', DoubleType(), True), StructField('extra', DoubleType(), True), StructField('mta_tax', DoubleType(), True), StructField('tip_amount', DoubleType(), True), StructField('tolls_amount', DoubleType(), True), StructField('ehail_fee', IntegerType(), True), StructField('improvement_surcharge', DoubleType(), True), StructField('total_amount', DoubleType(), True), StructField('payment_type', DoubleType(), True), StructField('trip_type', DoubleType(), True), StructField('congestion_surcharge', DoubleType(), True)])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you want to use schema elsewhere in your code\n",
    "df_green.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(VendorID=2, lpep_pickup_datetime=datetime.datetime(2019, 12, 18, 15, 52, 30), lpep_dropoff_datetime=datetime.datetime(2019, 12, 18, 15, 54, 39), store_and_fwd_flag='N', RatecodeID=1.0, PULocationID=264, DOLocationID=264, passenger_count=5.0, trip_distance=0.0, fare_amount=3.5, extra=0.5, mta_tax=0.5, tip_amount=0.01, tolls_amount=0.0, ehail_fee=None, improvement_surcharge=0.3, total_amount=4.81, payment_type=1.0, trip_type=1.0, congestion_surcharge=0.0),\n",
       " Row(VendorID=2, lpep_pickup_datetime=datetime.datetime(2020, 1, 1, 0, 45, 58), lpep_dropoff_datetime=datetime.datetime(2020, 1, 1, 0, 56, 39), store_and_fwd_flag='N', RatecodeID=5.0, PULocationID=66, DOLocationID=65, passenger_count=2.0, trip_distance=1.28, fare_amount=20.0, extra=0.0, mta_tax=0.0, tip_amount=4.06, tolls_amount=0.0, ehail_fee=None, improvement_surcharge=0.3, total_amount=24.36, payment_type=1.0, trip_type=2.0, congestion_surcharge=0.0),\n",
       " Row(VendorID=2, lpep_pickup_datetime=datetime.datetime(2020, 1, 1, 0, 41, 38), lpep_dropoff_datetime=datetime.datetime(2020, 1, 1, 0, 52, 49), store_and_fwd_flag='N', RatecodeID=1.0, PULocationID=181, DOLocationID=228, passenger_count=1.0, trip_distance=2.47, fare_amount=10.5, extra=0.5, mta_tax=0.5, tip_amount=3.54, tolls_amount=0.0, ehail_fee=None, improvement_surcharge=0.3, total_amount=15.34, payment_type=1.0, trip_type=1.0, congestion_surcharge=0.0),\n",
       " Row(VendorID=1, lpep_pickup_datetime=datetime.datetime(2020, 1, 1, 0, 52, 46), lpep_dropoff_datetime=datetime.datetime(2020, 1, 1, 1, 14, 21), store_and_fwd_flag='N', RatecodeID=1.0, PULocationID=129, DOLocationID=263, passenger_count=2.0, trip_distance=6.3, fare_amount=21.0, extra=3.25, mta_tax=0.5, tip_amount=0.0, tolls_amount=0.0, ehail_fee=None, improvement_surcharge=0.3, total_amount=25.05, payment_type=2.0, trip_type=1.0, congestion_surcharge=2.75),\n",
       " Row(VendorID=1, lpep_pickup_datetime=datetime.datetime(2020, 1, 1, 0, 19, 57), lpep_dropoff_datetime=datetime.datetime(2020, 1, 1, 0, 30, 56), store_and_fwd_flag='N', RatecodeID=1.0, PULocationID=210, DOLocationID=150, passenger_count=1.0, trip_distance=2.3, fare_amount=10.0, extra=0.5, mta_tax=0.5, tip_amount=0.0, tolls_amount=0.0, ehail_fee=None, improvement_surcharge=0.3, total_amount=11.3, payment_type=1.0, trip_type=1.0, congestion_surcharge=0.0)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the first 5 rows\n",
    "df_green.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns and Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-------------+\n",
      "| col1| col2|         Name|\n",
      "+-----+-----+-------------+\n",
      "|James| Bond| James & Bond|\n",
      "|Scott|Varsa|Scott & Varsa|\n",
      "+-----+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Concatenate columns using || (sql like)\n",
    "data = [(\"James\", \"Bond\"), (\"Scott\", \"Varsa\")] \n",
    "df = spark.createDataFrame(data).toDF(\"col1\", \"col2\") \n",
    "df.withColumn(\"Name\", expr(\"col1 || ' & ' || col2\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|   name| gender|\n",
      "+-------+-------+\n",
      "|  James|   Male|\n",
      "|Michael| Female|\n",
      "|    Jen|unknown|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"James\", \"M\"),(\"Michael\", \"F\"),(\"Jen\", \"\")]\n",
    "columns = [\"name\", \"gender\"]\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "# Using CASE WHEN similar to SQL.\n",
    "df2 = df.withColumn(\"gender\", expr(\"CASE WHEN gender = 'M' THEN 'Male' \" + \"WHEN gender = 'F' THEN 'Female' ELSE 'unknown' END\"))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------+\n",
      "|      date|increment|  inc_date|\n",
      "+----------+---------+----------+\n",
      "|2019-01-23|        1|2019-02-23|\n",
      "|2019-06-24|        2|2019-08-24|\n",
      "|2019-09-20|        3|2019-12-20|\n",
      "+----------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"2019-01-23\",1), (\"2019-06-24\",2), (\"2019-09-20\",3)] \n",
    "df = spark.createDataFrame(data).toDF(\"date\", \"increment\") \n",
    "# Add Month value from another column\n",
    "df.select('date', 'increment', expr(\"add_months(date, increment)\").alias(\"inc_date\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- increment: long (nullable = true)\n",
      " |-- str_increment: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using Cast() Function\n",
    "df.select('increment', expr(\"cast(increment as string) as str_increment\")).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------------+\n",
      "|      date|increment|new_increment|\n",
      "+----------+---------+-------------+\n",
      "|2019-01-23|        1|            6|\n",
      "|2019-06-24|        2|            7|\n",
      "|2019-09-20|        3|            8|\n",
      "+----------+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Arthemetic operations\n",
    "df.select('date','increment',expr(\"increment + 5 as new_increment\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|col1|col2|\n",
      "+----+----+\n",
      "| 500| 500|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use expr()  to filter the rows\n",
    "data = [(100, 2), (200, 3000), (500, 500)] \n",
    "df = spark.createDataFrame(data).toDF(\"col1\", \"col2\") \n",
    "df.filter(expr(\"col1 == col2\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VendorID',\n",
       " 'lpep_pickup_datetime',\n",
       " 'lpep_dropoff_datetime',\n",
       " 'store_and_fwd_flag',\n",
       " 'RatecodeID',\n",
       " 'PULocationID',\n",
       " 'DOLocationID',\n",
       " 'passenger_count',\n",
       " 'trip_distance',\n",
       " 'fare_amount',\n",
       " 'extra',\n",
       " 'mta_tax',\n",
       " 'tip_amount',\n",
       " 'tolls_amount',\n",
       " 'ehail_fee',\n",
       " 'improvement_surcharge',\n",
       " 'total_amount',\n",
       " 'payment_type',\n",
       " 'trip_type',\n",
       " 'congestion_surcharge']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return column names\n",
    "df_green.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(VendorID=2, lpep_pickup_datetime=datetime.datetime(2019, 12, 18, 15, 52, 30), lpep_dropoff_datetime=datetime.datetime(2019, 12, 18, 15, 54, 39), store_and_fwd_flag='N', RatecodeID=1.0, PULocationID=264, DOLocationID=264, passenger_count=5.0, trip_distance=0.0, fare_amount=3.5, extra=0.5, mta_tax=0.5, tip_amount=0.01, tolls_amount=0.0, ehail_fee=None, improvement_surcharge=0.3, total_amount=4.81, payment_type=1.0, trip_type=1.0, congestion_surcharge=0.0),\n",
       " Row(VendorID=2, lpep_pickup_datetime=datetime.datetime(2020, 1, 1, 0, 45, 58), lpep_dropoff_datetime=datetime.datetime(2020, 1, 1, 0, 56, 39), store_and_fwd_flag='N', RatecodeID=5.0, PULocationID=66, DOLocationID=65, passenger_count=2.0, trip_distance=1.28, fare_amount=20.0, extra=0.0, mta_tax=0.0, tip_amount=4.06, tolls_amount=0.0, ehail_fee=None, improvement_surcharge=0.3, total_amount=24.36, payment_type=1.0, trip_type=2.0, congestion_surcharge=0.0),\n",
       " Row(VendorID=2, lpep_pickup_datetime=datetime.datetime(2020, 1, 1, 0, 41, 38), lpep_dropoff_datetime=datetime.datetime(2020, 1, 1, 0, 52, 49), store_and_fwd_flag='N', RatecodeID=1.0, PULocationID=181, DOLocationID=228, passenger_count=1.0, trip_distance=2.47, fare_amount=10.5, extra=0.5, mta_tax=0.5, tip_amount=3.54, tolls_amount=0.0, ehail_fee=None, improvement_surcharge=0.3, total_amount=15.34, payment_type=1.0, trip_type=1.0, congestion_surcharge=0.0),\n",
       " Row(VendorID=1, lpep_pickup_datetime=datetime.datetime(2020, 1, 1, 0, 52, 46), lpep_dropoff_datetime=datetime.datetime(2020, 1, 1, 1, 14, 21), store_and_fwd_flag='N', RatecodeID=1.0, PULocationID=129, DOLocationID=263, passenger_count=2.0, trip_distance=6.3, fare_amount=21.0, extra=3.25, mta_tax=0.5, tip_amount=0.0, tolls_amount=0.0, ehail_fee=None, improvement_surcharge=0.3, total_amount=25.05, payment_type=2.0, trip_type=1.0, congestion_surcharge=2.75),\n",
       " Row(VendorID=1, lpep_pickup_datetime=datetime.datetime(2020, 1, 1, 0, 19, 57), lpep_dropoff_datetime=datetime.datetime(2020, 1, 1, 0, 30, 56), store_and_fwd_flag='N', RatecodeID=1.0, PULocationID=210, DOLocationID=150, passenger_count=1.0, trip_distance=2.3, fare_amount=10.0, extra=0.5, mta_tax=0.5, tip_amount=0.0, tolls_amount=0.0, ehail_fee=None, improvement_surcharge=0.3, total_amount=11.3, payment_type=1.0, trip_type=1.0, congestion_surcharge=0.0),\n",
       " Row(VendorID=1, lpep_pickup_datetime=datetime.datetime(2020, 1, 1, 0, 52, 33), lpep_dropoff_datetime=datetime.datetime(2020, 1, 1, 1, 9, 54), store_and_fwd_flag='N', RatecodeID=1.0, PULocationID=35, DOLocationID=39, passenger_count=1.0, trip_distance=3.0, fare_amount=13.5, extra=0.5, mta_tax=0.5, tip_amount=0.0, tolls_amount=0.0, ehail_fee=None, improvement_surcharge=0.3, total_amount=14.8, payment_type=1.0, trip_type=1.0, congestion_surcharge=0.0),\n",
       " Row(VendorID=2, lpep_pickup_datetime=datetime.datetime(2020, 1, 1, 0, 10, 18), lpep_dropoff_datetime=datetime.datetime(2020, 1, 1, 0, 22, 16), store_and_fwd_flag='N', RatecodeID=1.0, PULocationID=25, DOLocationID=61, passenger_count=1.0, trip_distance=2.77, fare_amount=11.0, extra=0.5, mta_tax=0.5, tip_amount=0.0, tolls_amount=0.0, ehail_fee=None, improvement_surcharge=0.3, total_amount=12.3, payment_type=2.0, trip_type=1.0, congestion_surcharge=0.0)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take first 7 rows\n",
    "df_green.take(7)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common DataFrame Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+------------+------------+---------------+\n",
      "|lpep_pickup_datetime|lpep_dropoff_datetime|PULocationID|DOLocationID|passenger_count|\n",
      "+--------------------+---------------------+------------+------------+---------------+\n",
      "| 2020-01-01 00:45:58|  2020-01-01 00:57:37|         145|         179|            6.0|\n",
      "| 2020-01-01 00:28:39|  2020-01-01 01:08:08|          80|         238|            6.0|\n",
      "| 2020-01-01 00:06:02|  2020-01-01 00:27:20|          66|         228|            6.0|\n",
      "| 2020-01-01 00:43:08|  2020-01-01 01:08:09|          66|         107|            6.0|\n",
      "| 2020-01-01 00:07:06|  2020-01-01 00:11:13|          41|         152|            6.0|\n",
      "+--------------------+---------------------+------------+------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Projections and filters\n",
    "df_green\\\n",
    "    .select('lpep_pickup_datetime', 'lpep_dropoff_datetime', 'PULocationID', 'DOLocationID', 'passenger_count')\\\n",
    "        .where((col('passenger_count') > 5) & (col('passenger_count') < 7))\\\n",
    "            .show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|DistinctPULocationID|\n",
      "+--------------------+\n",
      "|                 182|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count distinct cardinality of column\n",
    "df_green\\\n",
    "    .select('lpep_pickup_datetime', 'lpep_dropoff_datetime', 'PULocationID', 'DOLocationID', 'passenger_count')\\\n",
    "        .where((col('passenger_count') > 5) & (col('passenger_count') < 8))\\\n",
    "            .agg(countDistinct('PULocationID').alias('DistinctPULocationID'))\\\n",
    "                .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|PULocationID|\n",
      "+------------+\n",
      "|          29|\n",
      "|          26|\n",
      "|          65|\n",
      "|         222|\n",
      "|         243|\n",
      "|          54|\n",
      "|         112|\n",
      "|         167|\n",
      "|         155|\n",
      "|         113|\n",
      "+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List distinct cardinality of column\n",
    "df_green\\\n",
    "    .select('PULocationID')\\\n",
    "        .where((col('passenger_count') > 5) & (col('passenger_count') < 8))\\\n",
    "            .distinct()\\\n",
    "                .show(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Renaming, adding, and dropping columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------+\n",
      "|PuPuPuLocationID|DOLocationID|\n",
      "+----------------+------------+\n",
      "|             264|         264|\n",
      "|              66|          65|\n",
      "|             181|         228|\n",
      "|             129|         263|\n",
      "|             210|         150|\n",
      "+----------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rename column\n",
    "df_green\\\n",
    "    .select('PULocationID', 'DOLocationID')\\\n",
    "        .withColumnRenamed('PULocationID', 'PuPuPuLocationID')\\\n",
    "            .show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------------+\n",
      "|lpep_pickup_datetime|new_lpep_pickup_datetime|\n",
      "+--------------------+------------------------+\n",
      "| 2019-12-18 15:52:30|              2019-12-18|\n",
      "| 2020-01-01 00:45:58|              2020-01-01|\n",
      "+--------------------+------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert timestamp to date\n",
    "df_green\\\n",
    "    .select('lpep_pickup_datetime')\\\n",
    "        .withColumn('new_lpep_pickup_datetime', to_date(col('lpep_pickup_datetime'), \"yyy-MM-dd\"))\\\n",
    "            .show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------------+---------------------------+\n",
      "|lpep_pickup_datetime|new_lpep_pickup_datetime|new_lpep_pickup_datetime_ts|\n",
      "+--------------------+------------------------+---------------------------+\n",
      "| 2019-12-18 15:52:30|              2019-12-18|        2019-12-18 00:00:00|\n",
      "| 2020-01-01 00:45:58|              2020-01-01|        2020-01-01 00:00:00|\n",
      "+--------------------+------------------------+---------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert date to timestamp\n",
    "df_green\\\n",
    "    .select('lpep_pickup_datetime')\\\n",
    "        .withColumn('new_lpep_pickup_datetime', to_date(col('lpep_pickup_datetime'), \"yyy-MM-dd\"))\\\n",
    "            .withColumn('new_lpep_pickup_datetime_ts', to_timestamp(col('new_lpep_pickup_datetime')))\\\n",
    "                .show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+---------------------------+\n",
      "|new_lpep_pickup_datetime|new_lpep_pickup_datetime_ts|\n",
      "+------------------------+---------------------------+\n",
      "|              2019-12-18|        2019-12-18 00:00:00|\n",
      "|              2020-01-01|        2020-01-01 00:00:00|\n",
      "+------------------------+---------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop columns\n",
    "df_green\\\n",
    "    .select('lpep_pickup_datetime')\\\n",
    "        .withColumn('new_lpep_pickup_datetime', to_date(col('lpep_pickup_datetime'), \"yyy-MM-dd\"))\\\n",
    "            .withColumn('new_lpep_pickup_datetime_ts', to_timestamp(col('new_lpep_pickup_datetime')))\\\n",
    "                .drop('lpep_pickup_datetime')\\\n",
    "                    .show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 100:============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+-----+----+\n",
      "|lpep_pickup_datetime|date|month|year|\n",
      "+--------------------+----+-----+----+\n",
      "| 2009-01-01 01:27:08|   1|    1|2009|\n",
      "| 2009-01-01 00:33:51|   1|    1|2009|\n",
      "| 2009-01-01 00:27:33|   1|    1|2009|\n",
      "+--------------------+----+-----+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Use month(), year(), dayofmonth()\n",
    "df_green\\\n",
    "    .select('lpep_pickup_datetime')\\\n",
    "        .withColumn('date', dayofmonth('lpep_pickup_datetime'))\\\n",
    "            .withColumn('month', month('lpep_pickup_datetime'))\\\n",
    "                .withColumn('year', year('lpep_pickup_datetime'))\\\n",
    "                    .orderBy('month')\\\n",
    "                        .show(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|PULocationID|count|\n",
      "+------------+-----+\n",
      "|          75| 7464|\n",
      "|          74| 5962|\n",
      "|          41| 2498|\n",
      "|          43| 1653|\n",
      "|          42| 1438|\n",
      "|           7| 1055|\n",
      "|         166|  840|\n",
      "|          82|  809|\n",
      "|          95|  763|\n",
      "|         129|  676|\n",
      "+------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_green\\\n",
    "    .select('lpep_pickup_datetime', 'lpep_dropoff_datetime', 'PULocationID', 'DOLocationID', 'passenger_count')\\\n",
    "        .where((col('passenger_count') > 5) & (col('passenger_count') < 7))\\\n",
    "            .groupBy('PULocationID')\\\n",
    "                .count()\\\n",
    "                    .orderBy('count', ascending=False)\\\n",
    "                        .show(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read the datasets\n",
    "df = spark.read\\\n",
    "        .option('inferSchema', 'true')\\\n",
    "            .option('header', 'true')\\\n",
    "                .csv('departuredelays.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tempporary view\n",
    "df.createOrReplaceTempView('us_delay_flights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----------+\n",
      "|distance|origin|destination|\n",
      "+--------+------+-----------+\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Find all flights whose distance is greater than 1000 miles\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT\n",
    "        distance,\n",
    "        origin,\n",
    "        destination\n",
    "    FROM\n",
    "        us_delay_flights\n",
    "    WHERE \n",
    "        distance > 1000\n",
    "    ORDER BY \n",
    "        distance DESC\n",
    "    \"\"\"\n",
    ").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------+-----------+\n",
      "|   date|delay|origin|destination|\n",
      "+-------+-----+------+-----------+\n",
      "|2190925| 1638|   SFO|        ORD|\n",
      "|1031755|  396|   SFO|        ORD|\n",
      "|1022330|  326|   SFO|        ORD|\n",
      "|1051205|  320|   SFO|        ORD|\n",
      "|1190925|  297|   SFO|        ORD|\n",
      "|2171115|  296|   SFO|        ORD|\n",
      "|1071040|  279|   SFO|        ORD|\n",
      "|1051550|  274|   SFO|        ORD|\n",
      "|3120730|  266|   SFO|        ORD|\n",
      "|1261104|  258|   SFO|        ORD|\n",
      "+-------+-----+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Find all flights were between SFO and ORD with at least a two-hour delay\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT\n",
    "        date,\n",
    "        delay,\n",
    "        origin,\n",
    "        destination\n",
    "    FROM\n",
    "        us_delay_flights\n",
    "    WHERE\n",
    "        delay > 120 \n",
    "        AND origin = 'SFO' \n",
    "        AND destination = 'ORD'\n",
    "    ORDER BY \n",
    "        delay DESC\n",
    "    \"\"\"\n",
    ").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----------+-------------+\n",
      "|delay|origin|destination|Flight_Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "|  333|   ABE|        ATL|  Long Delays|\n",
      "|  305|   ABE|        ATL|  Long Delays|\n",
      "|  275|   ABE|        ATL|  Long Delays|\n",
      "|  257|   ABE|        ATL|  Long Delays|\n",
      "|  247|   ABE|        ATL|  Long Delays|\n",
      "|  247|   ABE|        DTW|  Long Delays|\n",
      "|  219|   ABE|        ORD|  Long Delays|\n",
      "|  211|   ABE|        ATL|  Long Delays|\n",
      "|  197|   ABE|        DTW|  Long Delays|\n",
      "|  192|   ABE|        ORD|  Long Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Label all US flights, regardless of origin and destination with an indication of delays\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT\n",
    "        delay,\n",
    "        origin,\n",
    "        destination,\n",
    "        CASE \n",
    "            WHEN delay > 360 THEN 'Very Long Delays'\n",
    "            WHEN delay > 120 AND delay <= 360 THEN 'Long Delays'\n",
    "            WHEN delay > 60 AND delay <= 120 THEN 'Short Delays'\n",
    "            WHEN delay > 0 AND DELAY <= 60 THEN 'Tolerable Delays'\n",
    "            WHEN delay = 0 THEN 'No Delay'\n",
    "            ELSE 'Early'\n",
    "        END AS Flight_Delays\n",
    "    FROM \n",
    "        us_delay_flights\n",
    "    ORDER BY\n",
    "        origin ASC,\n",
    "        delay DESC\n",
    "    \"\"\"\n",
    ").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
